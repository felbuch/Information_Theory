{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Musings on Entropy and Information Theory\n",
    "\n",
    "\n",
    "Author: Felipe Buchbinder\n",
    "\n",
    "\n",
    "Date: 10-10-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note:\n",
    "I like to learn by teaching myself. Thinking about how I would teach something helps me think about the most basic and fundamental ideas which underlie the topic I'm studying. I made this notebook for my own study.  I'm sharing it, but I don't intend it to be perfect nor absolutely rigorous. It's just the way *I* found it easier to understand these concepts. I enjoyed it. I hope you enjoy it too! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information of an event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information, like comedy, is all about surprise. \\\n",
    "\\\n",
    "If my mother tells me I'm handsome, it doesn't come as anything unexpected. It doesn't tell me much either. She always thinks I'm handsom (and mother's always right!)\\\n",
    "\\\n",
    "But if that gorgeous girl tells me I look really sexy right now, THAT_ is unexpected. It's also much more informative! It informs me I should stop writing these musings and go talk to the girl! \n",
    "\n",
    "It is highly probable that my mom would tell me I'm handsom. So, if this happens, it doesn't surprise me. Neither does it inform me anything about how handsom I really am. \n",
    "\n",
    "I'm sad to say, however, that it is very unlikely that a gorgeous girl will look me in the eye and call me the sexy protagonist of her wildest dreams. So if this does happen, I'm flabbergasted! It's also really informative: I do have a change with the girl after all\n",
    "\n",
    "Hence,  the amount of surprise (and information) that I gain from observing an event is small if this event is likely to happen anyway, but huge, if this event is unlikely. Let's put it this way: The surprise of observing an event $x$ which happens with probability $p(x)$ is \n",
    "\n",
    "$$\n",
    "S(x) \\equiv \\log_2 {\\frac{1}{p(x)}}\n",
    "$$\n",
    "\n",
    "This surprise is a measure of how much information I gain by observing this event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    return(np.log(x)/np.log(2))\n",
    "\n",
    "def information_of_event(p, digits = 2):\n",
    "    return(round(log2(1.0/p),digits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every morning I wake up and I feel, deep in my heart, that with 99% probability, mom will think me handsom. So if it does happen, my surprise (and the information that gives me) is only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_of_event(.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But a gorgeous girl calling me sexy is something whose probability of happening is 1 in 10,000. So if it does happen, my surprise (and the information that gives me) is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_of_event(0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much higher! \n",
    "\n",
    "So now, Mom, you know why I never listen to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A loving family of four"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability, surprise, information, and learning are 4 members of a loving family \\\n",
    "\\\n",
    "If an event has high probability, it doesn't surprise me much when it happens and it brings no information either. I learn nothing new I didn't know before\\\n",
    "\\\n",
    "If, however, an event which has small probability happens, it comes as a surprise and it is very informative. I learn something new! I update my beliefs! My priors! \n",
    "\n",
    "Think of the classic Black Swan example. How surprising it must have been to see a black swan for the first time! How informative! Not all swans are white after all! We've learned something we didn't know before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all have that friend who's insanely fun. You never know what to expect of him. So we know whenever we go out with him, we're in for a good time! Why is that?\n",
    "\n",
    "Well, suppose this friend is a random variable, and whatever he does are his events. Since we don't know what to expect of him, our expected value of surprise is high. And that's fun!\n",
    "\n",
    "Now, we all have that colleague at work who is just boring! Nothing new. Nothing different. He's so predictable! If he were a random variable, hist expected value of surprise would be really low.\n",
    "\n",
    "A random variable has events, and each event may surprise us, more or less. The greater our uncertainty on which event will actually happen, the greater the expected value of our surprise. This expected value has a name: Entropy.\n",
    "\n",
    "The Entropy of a random variable $X$ is the expected value of how much it will surprise us when an event actually happens. In other words, **the entropy is the expected value of information we will gain by observing $X$**. Mathematically,\n",
    "\n",
    "$$\n",
    "H(X) \\equiv \\mathbb{E}_p S(X) = \\mathbb{E}_p \\left[\\log_2 \\frac{1}{p(x_i)}\\right]\n",
    "$$\n",
    "\n",
    "If $X$ is a discrete random variable, then\n",
    "\n",
    "$$\n",
    "H(X) = \\sum_{i=1}^{N} p(x_i) \\log_2 \\frac{1}{p(x_i)}\n",
    "$$\n",
    "\n",
    "It is also useful to think of entropy in terms of **uncertainty**. The more uncertain we are about the results of a random variable, the more information we expect to obtain from observing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(list_of_probabilities, digits = 2):\n",
    "\n",
    "    assert all(p >= 0 for p in list_of_probabilities), \"All probabilities must be non-negative\"\n",
    "    assert all(p <= 1 for p in list_of_probabilities), \"Probabilities cannot be greater than 1\"\n",
    "    assert sum(list_of_probabilities) == 1, \"Probabilities should add 1\"\n",
    "    \n",
    "    entropy = 0.0\n",
    "    for p in list_of_probabilities:\n",
    "        if p > 0:\n",
    "            entropy += p * information_of_event(p)\n",
    "        elif p == 0:\n",
    "            #If p = 0, we make 0 * log(1/0) = 0.\n",
    "            #We can show this makes sense using L'Hopital's theorem,\n",
    "            #but we will not discuss it here.\n",
    "            entropy += 0\n",
    "        else:\n",
    "            raise ValueError(\"Invalid list of probabilities\")\n",
    "    return round(entropy, digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limiting cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there maximum and minimum values for a variable's entropy? \n",
    "\n",
    "Let's begin by discussing whether there is a minimum value. If the entropy measures the average surprise we have by observing a random variable, a minimum value for entropy happens when we have no surprise at all. This happens when $X$ is a constant. There's no doubt about the value of 5. It's 5. It's 5 with probability 1. If we calculate its entropy, we get\n",
    "\n",
    "$$\n",
    "H(5) = 1 \\cdot \\log_2 {1} = 0\n",
    "$$\n",
    "\n",
    "No uncertainty exists.\n",
    "\n",
    "So entropy has a minimum value, and it is zero.\n",
    "\n",
    "What about a maximum value for entropy? To ask whether there is a maximum value for the entropy is to ask whether there is a scenario when our uncertainty is the highest possible. And there is such a scenario.\n",
    "\n",
    "Our uncertainty is highest in a uniform distribution. After all, when everything has the same probability of happening, there's really no way of guessing what will actually happen. In this scenario, $p(x_i) = \\frac{1}{n} \\forall i$. This yields as a maximum possible value for an entropy, which is\n",
    "\n",
    "$$\n",
    "H_\\max(X) = \\log_2 n\n",
    "$$\n",
    "\n",
    "If I were the guy begind of this (i.e., if I were Shannon), I would have divided $H(X)$ by $\\log_2 n$, to make $H(X)$ be defined in the $[0,1]$ interval. But I'm not Shannon. So $H(X)$ is actually defined in the interval $[0, \\log_2 n]$. How can such a smart guy like Shannon make such bad choices in life?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3gc1dXA4d9RlyVbsizJlqvcexemGTDGgAvg0E2A0AIhBEgghFASEiABAgES8tEMIdTQDASbmgA2LrjJ3XK3XCTLRbaa1dv9/piRvJJVVrJWs+W8z7OPZ2dmd8+OV3v2ztx7rhhjUEopFbiCnA5AKaWUszQRKKVUgNNEoJRSAU4TgVJKBThNBEopFeBCnA6gpeLj401ycrLTYShVR3p2EQD9EqIcjkSphq1ateqwMSahoW0+lwiSk5NJTU11Ogyl6rjy5aUAvP+zUx2ORKmGiciexrbpqSGllApwPtciUMob3TF5oNMhKNVqmgiUagMTB8Y7HYJSraanhpRqA2lZ+aRl5TsdhlKtoolAqTbwyLxNPDJvk9NhKNUqHksEIvKaiBwSkY2NbBcReU5EdojIehEZ56lYlFJKNc6TLYLXgalNbJ8GDLRvtwAvejAWpZRSjfDYxWJjzEIRSW5il5nAm8aqg71MRGJFJMkYs7+p503PLqrts13jglFJXHtqMiXlVVz/rxXHPeay8T25PKUXOUXl/PztVcdtv+aUPlw4ujtZeSXc9f7a47bffEY/pgzrys7sQh74eMNx2++YPJCJA+NJy8pv8PTAvVMHM75PHKv25PDkV1uP2/7QhcMY3j2GxdsP84/vth+3/bFLRtI/IZpvNh3klUXpx21/9soxdI+NZN66LN5ednxX4RevGU9cVBgfpmYwZ1Xmcdtfv2ECkWHBvLV0N5+tP/7w1/SNn71wJ99uPlRnW0RoMG/cOAGA577dzpIdh+ts79whjJeuHQ/AX77awuo9uXW2J8VE8LdZYwF4eF4am7IK6mzvlxDF45eMAuD+j9fXDtyqMax7J/5w4XAAfvXeGvbnl9bZPq5PZ347dQgAt761itzi8jrbTx8Qz53nWD1+rnttBXtziiivNIQGC/klFUSFhzCoa0fio8OOe28AAxM7MqpnDHtzijhSWM7pT3xbZ/vQpE4MS+pESXkVX2zcT0WVobyymqpqQ1CQcN6wriTFRhIksHTnkeOeXz97gfPZK62oqrP9nKGJ3HJmf4DjvvPgxL/3XDnZa6gHkOFyP9Ned9ynQURuwWo1EJ3Uv12CU4GnvLKaXYeL66zLLa4gM7ek0cfsyyvl++3Z1EzrsS+v9Ljt3245RGPTfnzo8uU4qmcMkaHBrQteqRMgnpyYxm4RfGaMGdHAts+Bx40xi+373wL3GmOaTF0pKSlGRxar1jhSWMa/l++luKKKbzYdpKSiigtGda/dnl9SzrsrMhp9/JZHpxLRyBf1qj05AIzvE9dkDMn3fd7otovH9qBrp4ja+1+nHQBg2ohuhAYHceVJvegeG9nk8yvVGBFZZYxJaWibky2CTKCXy/2eQJZDsagA8PayvTz7zbY66176fidhIc1fKhvSrWOjSQCaTwA1TukXx7L0nAa3fb7hWGO4sqqaavs32gsLdgJw6Ghp7akKpdqSk4lgLnC7iLwHnAzkN3d9QCl3bD1wlPdXZhAcBD/sPMIp/boA8P227OP2feKSkcya0PuEX9PdFsF7t7hXi+iHnYf58SvL66z7fP1+OoRZf7LLdx3h1H5dqKgyXDy2B6N7xbYiaqUsHksEIvIuMAmIF5FM4A9AKIAx5iXgC2A6sAMoBm7wVCwqsPxxbhpL049deN1+qJCw4CCqqo8/DXrmoAaLMbZYzYXYtio6Nzwp5rh1ReVVvL8yg4qqasoqq9m4z7q4uXBbNt/dM6lNXlcFJk/2Grqqme0G+IWnXl8FhuXpR/hwVSaRocGk7snlpOTOdZIAWL/6LxnX06EIWyemQyi7n5jR4LaF27L5yWvHeomkHy7ioU83smpPLmN7x1JcVsVFY7ozaXBie4WrfJzWGlI+7e4P1rEv71ivnj1HiggJEirtX//dYyIY0eP4X9e+bGDXaHrFRZKRc+x9f7x6H4VllaTZXSDnrstix2PTnQpR+RhNBMqnrN6byz0friM9u4irJvSukwQAnrliNFNHJDkUXftIiolk0b2T66yrf02hstpw/8cb+Hh1Jp07hPHMlaM5rb8WxlMN01pDyqfc/9GG2oE9X23cX6fHT3KXDgxN6uRUaI4amNjxuNnR/pt2gLLKag4UlHLbO6sdikz5Am0RKK+2ak8u985Zx87sIi4d15OtB4/Wbvvw1tMYkBjtYHTHPHThMEdfP6FjON/9elKddXnF5Yx55H/2cgW//mAdH63OJLlLB/70o5FaOlvV0haB8mp/+XILO+0WwOcbsogKO9aXv1ec9wyuGt49huHdvetaRMeI0Dr3567bB8DuI8Xc/8l6J0JSXkpbBMqrfL8tmz1Hiti4L5+yympW7D42+Oqdn57C+D6dHYyucYu3W3VuvOlXdnCQ1Ol5tGpPDpe+aNWsycgp4ZfvrSEkKIgxvWNJiA5n6ohuToWqHKaJQHmV61y6RfbsHEmP2MjaC8LDu3vv+f+agm3elAjqc22xxEeHs2ZvHntzivlotVXvqLHuqsr/aSJQjjtSWMZDn6axLjOvzvrP7phIbIcwh6LyPxGhwXW+7KuqDf0f+KL2/uSnF5DcJYpHfzSCHlrTKKDoNQLluOW7cvh8w/46VT7PGZJIp3rnuFXbCg4SZo45VnQvPbuI77Yc4vutx5fiUP5NWwTKEUcKy7jh9ZWsz8xnmEuXz//84nTGaN2cdvP3WWP5+6yx7D5cxKS/LgDg9R928cAnG+iXEMVbN52srYMAoC0C5YjUPbmsz7Qme6+2S6FPSI6jf72+8Kp9dI+N5Cy77lJIkPW1kJ5dxJLtx0/Go/yPR+cj8ASdj8B3lVdWc8e7q/k67SBDunVkywFrTMD2P08jNNi3f5PszC4EoH+Cd4xrOFE18yYMSIxmx6FCTuvfhdk/SSE6XE8i+Kqm5iPw7b8+5VPSDxfyddpBAHKKygkSuHRcT59PAmAlAH9JAgA3TewLQJBY93/YeYS0ffkORqQ8SVsEyqOMMZz91wXsPlJMbIdQ8oorAHjpmvF+1W/9m01WgpsyrKvDkbSttKx8Zjy3GLBGL2cfLaNjeAirHzrXLxJ4IPHWGcpUACgoqWT3EWse4LziCvrFR9EtJoIzvLi/fWvUTOzub4lgUNeOnDesK2lZBbXjOY6WVZKVV0KfLno9x19oIlBtzhjD9OcWs3l/wXHbdAIV3xIaHMTsn1g/Ik9+7BsOFpQBcNZTCwCrzMfC35yNiDgVomoD2rZTba6koqrBJPDC1eMciEa1lScvG33cuoycEorKqxyIRrUlbRGoNrU2I48/zk2rs05LF/iHswYl1P5f1vQqArj61eXce/5gTh/gX6f7Aom2CFSbWrQtm7UZx0pFTBrcNnMCK+9y2fhjU3+m7cvnuy2HHIxGnSjtNaRO2Fcb93Pr28cmPokIDWLLo9McjKj9ZdkXUrsH4CjcUx77lgMFpbX3n7psFJen9HIwItUQHUegPMo1CQD8YtIAhyJxTvfYyIBMAgA3n9mvzv3fzNG5DnyNJgJ1Qu5+f22d+wMTo7njnIEOReOceeuymLcuy+kwHHHTxL6c0i+uzrqfvpFKdbVvnW0IZJoIVKtVVxv+s3Yf8dFWqeg+XTrw6nUNtjz93tvL9vD2sj1Oh+GYZ64YQ++4DgD0iI3km80HOVpW6XBUyl3aa0i1SGVVNQMe/LLOulvO7MctZ/Z3KCLlDbrHRrLw3rMB+CA1g3vnrGf0w/+t3b7l0alEhAY39nDlMG0RqBY5dLSszv1Lx/Xk/OH+UypCnbizBiVwRUrPOuu22gUGlXfSRKDc9vn6/Vz64g911j19xWgtNaDq6Nop4rjBZ9f/awXvrdjrUESqOZoIlNt+2HmY3OLy2vtv3TTBwWiUt5tz66kE2+VLK6sNi3bo3AbeSscRqGaVVlRx6Ys/sONQId1jI5mv9YKOk1NkJci4KJ1juSEzn1/C5qwCenfpwJxbT9W5qB2g4wjUCcnKKyEtq4DxfTpz97mDnA7HK8VFhWkSaMKdkwdw+oAu7DhUyK7DRU6Ho+rRXkOqQbsOF3G2PYdtjZsm9uWcof5VZrmtfJiaAaAjahtxztCuxHYIZf7WbC5+4dh1pnm3T2RkzxgHI1OgLQLViOXpR+rcv/Ws/pzcr4tD0Xi/OasymbMq0+kwvNrIHrHcfnbdUedfbtzvUDTKlUcTgYhMFZGtIrJDRO5rYHtvEZkvImtEZL2ITPdkPMo9d767hgf/s7HOuvumDdH5atUJCQsJ4p7zB5PcpUPtuhcW7OQnr61wMCoFHkwEIhIMPA9MA4YBV4nIsHq7/Q74wBgzFpgFvOCpeJT7lu86wsDEaCJDgwkNFj6/c6LTISk/8tr1JxEdHkJwkDC6V+xxrU/V/jz5E28CsMMYkw4gIu8BM4FNLvsYoJO9HAMEZrEWLzF/6yF+8c5qisurmDmmBw9MH+p0SMoP9UuIZuPD5wPw/PwdrMvIY/DvvuTJy0Yxc0wPh6MLTJ48NdQDyHC5n2mvc/VH4BoRyQS+AO5o6IlE5BYRSRWR1OzsbE/EqrDqyheXV3HbpP5cfXJvp8NRAeCScT24Y/IAjIENmflOhxOwPDaOQEQuB843xvzUvn8tMMEYc4fLPnfbMTwtIqcC/wRGGGOqG3teHUfQ9iqqqpn89AIyckqIDA1m86NTnQ7J55TY0zVGhmk9ndY47fFvycovJT46nPn3nEXHiFCnQ/I7To0jyARc+9L15PhTPzcBHwAYY5YCEYDOd9fODheWkZFTwuQhiTx9xfHz0qrmRYYFaxI4AX+6eAQzRibVfhZ9baCrr/NkIlgJDBSRviIShnUxeG69ffYC5wCIyFCsRKDnftrRnz7bxKmPfwfAxWN7MH1kksMR+aa3lu7mraW7HY7Cd00e0pVZE6zfjdOfW6ST27QzjyUCY0wlcDvwNbAZq3dQmog8IiIX2bv9GrhZRNYB7wLXG/0p0G7KKqvYsC+fnp0jeXD6UCYPSXQ6JJ/12fr9fLZe+8SfiAl943jogmEMSIxm4758SiuqnA4pYHi0Y7gx5gusi8Cu6x5yWd4EnO7JGFTDNmTmc/ELS6isNkwZ2vW46QaVam/hIcHcOLEvm/YXMGdVJsMe+oo3bzyZiQP1bLGn6cjiAFRRVc2GfflUVhtuPas/vzl/sNMhKVXr9rMHcNeUQVQb2LAvn/LKRvuOqDaiiSAATfv7Ih74ZANgzS42uFtHhyNS6pjk+Ch+Pqk/wUHCX77awhlPfkeVzn/sUZoIAogxhkNHS9mZXchZgxJ48epxWjFTeaWwkCBmXzueqcO7cbCgjD1HiqjWZOAxWjwmgDz19VZeWLATgLMHJzBNewi1mfd/dqrTIfidc4Z2paC0gq/SDjD56e/58cm9eezikU6H5Zc0EQSInKJyNu0voGuncO6aMojpozQJKO83dXgSFZcaXvx+J1v2F3C4sIz46HCnw/I7emooAKRl5TP+T/9jwdZskrtEMWtCbzrpyM02NXvhTmYv3Ol0GH4nMiyYK07qxeCuHVm9N4+UP33DDzt1ysu2ponAzxWWVbJmbx7GwF1TBvHni0c4HZJf+nbzIb7dfMjpMPzWA9OH8qBdBHFtRh4FpRUOR+RfNBH4uel/X8Tv7LkFZk3oxYBE7SGkfE/vLh24+pTeBAk8+dVWznxyvvYkakOaCPxURVU1Ow4VkpFbzHnDuvLGjRPo2inC6bCUarUOYSG889NT+NGY7uQVV7BxXz5llTr6uC1oIvBTj362iSnPfI8xcPqAeM4alOB0SEqdsFP7d+FsuxTKzOeXcM+HWpOoLWgi8DPGGNKzC9ly4Ci94zrwwtXjuDylp9Nh+b2I0GAiQrX6aHuYOqIbL10zniHdOrLjUCE7Dh3VaqUnSBOBn5m/9RCTn/6eFbty6JcQxfSRSXQI017CnvbGjRN448YJTocREMJDgpk6ohtDunVk8/4CpjyzkE/W7HM6LJ+micCPZOQUs3Cb1bXuyctG8fglOvhG+a8HZwzj+R+PA2DJjiPsPlzkcES+SxOBHznjyfm8/sNuwoKDmDmmO0kxkU6HFDCe+3Y7z3273ekwAkpCx3BmjEqiU0QIH63OZNJfFzgdks/SROAndhwqrF3+5BenER6i56vb05Idh1myQwc6OWHOz0+rXV6XkedgJL5LE4GfmPLM97XLw7vHOBiJUu1rUNdjY2NmPr9Exxe0giYCP3CwoLR2edn95zgYiVLOWPHAsc+9a+tYuUcTgR+47Z3VtcvdYnTQmAo8iS6DJW9+M9XBSHyTJgIfVlJexeLth8nKKyFI4KtfneF0SAGrc4cwOnfQuR2c9M3dZ9EpIoSconIWbc/mqNYjcpsmAh/2yqJ0rvnncvbnl3LTxL4M6dbJ6ZAC1kvXjuela8c7HUZAG5AYzc/O6k9hWSXX/nMFz/xvm9Mh+QxNBD6outqwdOcR1uzNJSYylI9+fhq/Pk/nHVbq5jP68fFtp9EjNpK0rAIWbz9MZZXOedwcTQQ+aNXeXK56ZRnzt2bTs3Mk4/t01vIGDvvLV1v4y1dbnA4j4IWFBDGud2f6xkexYlcO1/xzOQu2ZjsdltfTROBjth08ys/fti4OP3fVWN6+6WSHI1IAq/fksnpPrtNhKNvzV4/jXzecBMDP3l7FlgMFDkfk3TQR+Jjznl3I4cIyACYOiKezTj6v1HFiIkM5tV8XAKqqDVP/tsjhiLybJgIf9eLV44jTJKBUoyJCg3nx6nG197VCaeM0EfgQ14FjY3t3djASpXzDuD7H/k6260CzRmki8BFbDxzlw9QMAH5/wTAdOOZlkmIiSNL/E6/TtVMEz145GoD/rNnHxn35DkfknbRQvY+4+4O1pGVZF7xO7hvncDSqvr/NGut0CKoRKX3iEIEXFuzk07VZLLlvstMheR1tEXi5vOJyPkjNYF9eCTNGJrHiwXMY0UOLyinlrl5xHUh9cApXTehF9tEy3l+5l0NHS5t/YADRRODl3lq6h3vnrCevuIJh3TuR2FFPP3ijh+el8fC8NKfDUI3oEh3OyB6xlFdV89uPNvDy9+lOh+RVPJoIRGSqiGwVkR0icl8j+1whIptEJE1E/u3JeHzRM99Yw+SX3X8Ot03q73A0qjGbsgrYlKV91b3ZVRN6seKBc4iLCuOfi3c5HY5X8dg1AhEJBp4HzgUygZUiMtcYs8lln4HA/cDpxphcEUn0VDy+6GBBKTU93vTisFInRkRI7BRBTlE5YA3OdJ3LIJB5skUwAdhhjEk3xpQD7wEz6+1zM/C8MSYXwBhzyIPx+JSjpRX8e/lep8NQym+9vWwPuXZSCHSeTAQ9gAyX+5n2OleDgEEiskRElonI1IaeSERuEZFUEUnNzg6MuiFfbjjA3+05cFP66JgBpdrKhaO7A/Dm0j3MWZXpcDTewZPdR6WBdfWH9oUAA4FJQE9gkYiMMMbUmXjUGDMbmA2QkpLi98MD73p/LQfyrV4Nqb+bQnx0uMMRqeb0S4hyOgTlpn9cNZYnLx3FyD9+zafr9rF8Vw6zrx1PUFBDX1mBwZOJIBPo5XK/J5DVwD7LjDEVwC4R2YqVGFZ6MC6vtjYjj0/W7AOgV1wkXbSMhE94/JJRToegWiAyLJghSR3ZuK+AjfsKWJp+hNMHxDsdlmPcOjUkIheISEtPI60EBopIXxEJA2YBc+vt8x/gbPs14rFOFQV0v67N+4/1PFlwz9mIBO6vFKU86dNfTKxdDvTKse5+uc8CtovIkyIy1J0HGGMqgduBr4HNwAfGmDQReURELrJ3+xo4IiKbgPnAb4wxR1r2FvzL4aNWZdGrT+5NcAA3VX3N/R+v5/6P1zsdhmqB4CDhrimDAMgpDuyLxm4lAmPMNcBYYCfwLxFZal/AbbLvlTHmC2PMIGNMf2PMn+11Dxlj5trLxhhztzFmmDFmpDHmvRN8Pz7tw9QMluw8DMAjM0c4HI1qifTsItKzi5wOQ7XQL6cMpGN4CGv25vHW0t0BW6HU7dM9xpgC4COsbqBJwMXAahG5w0OxBZSiskp+M2c9y9JzGNkjRlsDSrWTMb1jWZuRx+8/TSMrPzBLT7h7jeBCEfkE+A4IBSYYY6YBo4F7PBhfQKiqNox79H8APHHJSObdMbGZRyil2spbN51cO2/B6U98R0l5lcMRtT93WwSXA88aY0YZY56qGfhljCkGbvRYdAHi6f9upazSmmB7SFInh6NRKvAMdBlh/PR/tzoYiTPcvUbwE2CbiFxktw66uWz71mPRBYhXF1l1T/olRDGmV6zD0ajWGNa9E8O6axL3VQMSoxlq/wh7NQDrELl7augmYAVwCXAZsExEtCXQRsqrrNbA7y8Y5nAkqrX+cOFw/nDhcKfDUCfgdzPc6hDpl9w9NXQvMNYYc70x5jpgPPBbz4UVGNZm5PHQpxsJCwnixtP7cvZgrbmnlFNOHxBf2530oU83snRn4PRkd3dkcSZw1OX+UerWEVKt8NriXXy2PovOHcIYr/WEfNqv3lsD6Exlvm5s71jio8N5Z/le9hwp5tT+XZwOqV24mwj2ActF5FOsekEzgRUicjeAMeYZD8Xnt4wxzF2XRY/YSJ06zw/sD9Buh/7mzEEJpP5uClP/tpDvt2VjjAmI0f3uJoKd9q3Gp/a/Wsy7lTbus0pJ7MsrcTgSpVR9Ww5YJ0C2HjzKkG7+3wnArURgjHkYwB5JbIwxhR6Nys+ty8jj8S83Ox2GUqoZD3y8gd+cP8TvTxG522tohIisATYCaSKySkS0i0Qrvbcyg2XpOQDcN22Iw9Eopeqr6UG0em8eby7d7Wgs7cHdU0OzgbuNMfMBRGQS8Apwmofi8lv5xRW8u2IvveM6sPDes50OR7WRcXqx36/89Ix+/PSMfkz7+yK+3HiA/fklJMVEOh2Wx7jbfTSqJgkAGGMWADoTRyv84t+rAdibU+xwJKot/XbqEH47VVt3/qamLPxd7691OBLPcjcRpIvI70Uk2b79Dgi84XcnaNvBoyzeYVUXrZkuTynlvWadZM2ttSw9h3UZec3s7bvcTQQ3AgnAx/YtHrjBU0H5qy83HKhd/vW5gxyMRLW1W99axa1vrXI6DNXGfjXl2N/pf9buczASz2o2EYhIMPCAMeZOY8w4+/YrY0xgT+nTQpuyCnj2m22EBgu7n5hBcryeWfMnucXl5Ab45Cb+qFtMBLufmEFCx3D+tWQ3K3fnOB2SRzSbCIwxVVglJdQJmP7cIgAE/x+copS/iQi1viovf2mpw5F4hrunhtaIyFwRuVZELqm5eTQyP5LhcmH4zxfrzGNK+Zpfnzu4dnn7waNN7Omb3O0+GgccAVxrIRis6wWqGa7Fqy4YpReJlfI15w+vrbzPgq3ZdeYv8AfuJoJXjTFLXFeIyOkeiMfvHCoo5d6PrEnN1/3hPCLDgh2OSHnC6QPinQ5BeVBkWDDpj02n3wNf8OcvNnPusK5+dZ3P3VND/3Bznapn1R7rmvr4Pp3pFOFu3lW+5s5zBnLnOQOdDkN5UFCQMGNkEgBL0/2rRHWT30wicirW6OGEmkqjtk6A/rRtRm5RObfZA8ieu2psQFQxVMqf/eWyUXy+YT/3f7yBswYl0D3WP0YbN9ciCAOisRJGR5dbAdZMZaoJG7PyMQYGd+1It04RToejPOi611Zw3WsrnA5DeVhUWDAT7dOAq/f6Tw/6JlsExpjvge9F5HVjzJ52iskvlFVWcds7x1oDwUHaGvBnpRVVToeg2oGI8NfLR3PK499y+7/XcNagBDpGhDod1glz96R1uIjMBpJdH2OM0RlVGpGRU8LR0kq6RIXR148uKikV6BI7hjMgMZodhwrZdrDQL2YXdPdi8YfAGuB3wG9cbqoBxeWVXP3qMgD+evlowkLcPcxKKW8XFCQ8edkoAK5+dRk5Rb4/otzdFkGlMeZFj0biR9KzizhYUEZ8dDhje8c6HY5Sqo0N7daJ/glR7MwuYsv+Ak7z8e7D7v5UnScit4lIkojE1dw8GpkPu3eONW7guavGENshzOFoVHs4Z2gi5wxNdDoM1U4iw4L5x1XjAPjD3DSHozlx7rYIrrP/dT0dZIB+bRuO7zPGsMmuYT66p7YGAsUtZ/Z3OgTVzgYkRgOw+0iRw5GcOLdaBMaYvg3cNAnUU1ZZxRUvW0Wp7ps2hKhwHUCmlL8KCwninvMGUVFlmPl/iykorXA6pFZrMhGIyL0uy5fX2/aYp4LyVfvzSlm5O5eT+8Yx1aU2ifJ/V768lCtf9s/KlKpx00YmccbAeNZl5pOe7bstg+ZaBLNclu+vt21qc08uIlNFZKuI7BCR+5rY7zIRMSKS0txzerPP1mcB1nyn/lSHRCnVsP4J0fzSLi3y8epMh6NpveYSgTSy3ND9uhutCW2eB6YBw4CrRGRYA/t1BO4EljcbrZfbl1cKwIS+eh1dqUAxokcMAIcLyxyOpPWaSwSmkeWG7tc3AdhhjEk3xpQD7wEzG9jvUeBJoLSZ5/NqN/xrBR+vzqRfQhQxkb4/0lAp5Z6I0GDG9Y7lm02HuOzFH6iubu6r0fs0lwhGi0iBiBwFRtnLNfdHNvPYHkCGy/1Me10tERkL9DLGfNbUE4nILSKSKiKp2dnZzbxs+6uqNszfms3gbh25W+ciVirg3D55AKN6xpC6J9cnLxo3V2voRCqMNnTqqDZVikgQ8CxwfXNPZIyZDcwGSElJ8bp0m2fPVXvR6O468UyAumBUktMhKAdNHtKVw4XlpO7JJftomc+NH/Jk/8ZMoJfL/Z5Alsv9jsAIYIFdnrkbMFdELjLGpHowrjaXW2z9AgjRwnIB69pTk50OQTksItT63VxQWulwJC3nySI4K4GBItJXRMKweiDNrdlojMk3xsQbY5KNMcnAMsDnksDXaQf40fPW5G19tKdQwCopr6KkXCuQBrJena25Ca55dTkfpmY0s7d38VgiMNeUReUAABeVSURBVMZUArcDXwObgQ+MMWki8oiIXOSp121vazPyKKmo4s7JAzilbxenw1EOuf5fK7j+XzofQSAb0SOGu6YMQgTWZOQ5HU6LeLQspjHmC2PMIGNMf2PMn+11Dxlj5jaw7yRfaw0YY3hxwU46hAVz93mDdT5ipQJYaHAQv5wykISO4fx7+V7KKn2nhaj1kU9AsX0qYFTPGIcjUUp5i5oaY0cKfac8tSaCE3DZS1ZJgQu1p5BSyjZ9pFVeZsZzixyOxH2aCFqputqw9UABkaHBTB2hdYWUUpYzBiaQ2DGc3OIKn5nCVBNBK834x2KqjTWQxNf6DKu2d9n4nlw2vqfTYSgvEBUewp12/aHTnvjOJ0YaayJohepqw+b9BcR2COXyFP3jV3B5Si8uT+nV/I4qIFwwKolecZHkFJVztMz7xxVoImiF9fvyAbjlzH4kdoxwOBrlDXKKyv1i7lrVNmI7hHHbpAEApNnfF95MZ05phfTsQgDG9e7scCTKW/z87VUAvP+zUx2ORHmLId06ArAvr8ThSJqnLYIW+mRNJnd/sA6AfjqSWCnViJ6dOwDwmznr+deSXQ5H0zRNBC20Kcuaj/j/fjyWxE56Wkgp1bCEjuE8/+NxRIUFs3FfgdPhNEkTQQu9smgXHcKCtcqoUqpZM0YlERMZykerMzHGe3sPaSJogSq7G9j4PnptQCnlnpRka8bCghLv7T2kiaAF7vtoPQBnDUpwOBLlba45pQ/XnNLH6TCUFzpjYDwAt7+72uFIGqeJwE1FZZXstHsLXXGS9hdXdV04ujsXjtbThep4M0YlERIkbNyXT3G5d7YKNBG46ZIXfmD13jzOH96VThE6J7GqKyuvhCwf6Cao2l+HsBAuHdeT3OIKLrfrk3kbTQRuysor4cxBCfz+gmFOh6K80F3vr+Wu99c6HYbyUr8+bxBxUWFeO6ZAE4GbSiurGNG9U23fYKWUcldipwguHdeD8spqp0NpkCYCNxw6WkpFlSEyVCeeUUq1TmRoMMXlVRw6Wup0KMfRROCGGc8tBiCmg14bUEq1ToxdpXjSUwtqu6J7C00EzdifX0L20TJG9ojhCq0uqZRqpR9P6M3pA7pQXF7F9kNHnQ6nDk0EzTjvmYUAXJHSkwg9NaQacfMZ/bj5jH5Oh6G8WGRYMLNO6g3A1L8t8qpWgSaCJlRWVXO0rJKTkjtrrXnVpCnDujJlWFenw1BebuqIbrUDUr1p9jJNBE0ota/wnzusq7YGVJN2ZhfWDjhUqjGhwUFMGZoIQIkmAt9QM6GE9hZSzXng4w088PEGp8NQPqDmR+X6zDyHIzlGE0EjSiuquHL2MgDiosIdjkYp5S+6RFu9h258PZUjhWUOR2PRRNCItRlWtr50XE+mjujmcDRKKX8xaVAiN5/RF4Cl6UccjsaiiaAR1722AoDpI7sRHCQOR6OU8hdBQcLUEUkA3P7vNVRUOT/aWBNBA4wxlFVWM3lIImcPTnQ6HKWUnxnXO5bLxvcEoLjc+YvGOnl9A8rs3kLj+3QmSFsDyg13TB7odAjKh4gIY3vHMmdVJqUVVcREOlu1QFsEDVhnXx/Q3kLKXRMHxjPRnoBEKXfUfL+s3J3jcCSaCBp085upACR20t5Cyj1pWfmkZeU7HYbyIV07RQDWdQKnRxl7NBGIyFQR2SoiO0Tkvga23y0im0RkvYh8KyKOz/W341AhBaWVzBiZxIyRSU6Ho3zEI/M28ci8TU6HoXzIaf27cM0pVsmJVXtyHY3FY4lARIKB54FpwDDgKhGpP6vLGiDFGDMKmAM86al43PWr99cA1jyjInp9QCnlGSLCxAFWuYk7HJ7P2JMtggnADmNMujGmHHgPmOm6gzFmvjGm2L67DOjpwXjcsvXAUU5K7syVOi+xUsrDzh/elXOHdeVggbMDyzyZCHoAGS73M+11jbkJ+LKhDSJyi4ikikhqdnZ2G4ZYV3W1oaLK0LVThLYGlFIeJyJ0j7GuFRSUVjgWhycTQUPfpA1eERGRa4AU4KmGthtjZhtjUowxKQkJCW0YYl1pWQUAjO4Z67HXUEopV8N7xACQ6mDvIU+OI8gEXM+v9ASy6u8kIlOAB4GzjDGOto+e+d9WAJLjo5wMQ/mge6cOdjoE5aOSu1jfN3/7ZjuThzhTytyTiWAlMFBE+gL7gFnAj113EJGxwMvAVGPMIQ/G0qzDhWXM35rN0KROnKt15VULje8T53QIykdN6BvHKf3iWJaew/78EpJiIts9Bo+dGjLGVAK3A18Dm4EPjDFpIvKIiFxk7/YUEA18KCJrRWSup+Jpzhs/7Aasod9KtdSqPTms2uP8wCDlm1LsHxIvLtjpyOuLMd4zXZo7UlJSTGpqaps/76Sn5pORW8L2P03TshKqxa58eSkA7//sVIcjUb7IGEPKn76hyhjW/P5cj3RWEZFVxpiUhrbpyGIgv7iC3UeKiQ4P0SSglGp3IkJMh1DyiivIzC1p99fXRAAcKCgF4MEZQx2ORCkVqO6bOgSAvTnFzezZ9jQRANsPHQUg1uEKgEqpwJXQ0aptln64qN1fWxMB8O6KvQAM697J4UiUUoFqQGI0AO8s20N7X7sN+PkIcovKWbLDmi4uPlqrjarWeejC+mW0lGqZ6HDr63jLgaPsPlJM33YczxTwLYIvNx4A4OnLRxOh8w+oVhrePYbh3WOcDkP5MBHh1Z9YnXo+WbOvXV874BPB6z/sAmBwt44OR6J82eLth1m8/bDTYSgfN7CrdXro38v3tuvrBnQiMMaw7WAhl4zrwYge+mtOtd4/vtvOP77b7nQYysf16RLFDacnc7iwjMp2nNQ+oBPB7iNWN62w4IA+DEopL1JzrWDDvvab8S6gvwFrDvT5w7s5HIlSSlkmDU4EYM3evHZ7zYBOBFv2W2Wna7ptKaWU02p6C20/VNhurxmwiaCyqppXFqXTMSKEXnEdnA5HKaUAiIsKo0+XDry7Yi+lFVXt8poBO45g68GjVFQZenXWsQPqxD12yUinQ1B+JCkmgj1Hilm9N5fT+sd7/PUCtkXwH7uf7p8uHuFwJMof9E+Ipn+CnmJUbeP3F1gDFD9YmdHMnm0jYBPBDzut0cQDE3X8gDpx32w6yDebDjodhvITfexZy1buzm2X1wvIRFBaUUVaVgE/GtO9ttCTUifilUXpvLIo3ekwlJ+IDg/hulP7sC+vpF0mtQ/IRJCWZXUbjYvSJKCU8k5JsdaUlSt3eX7mu4BMBOnZVpnXC0cnORyJUko17Dx77vQd7dCNNCATQU2XrC7aIlBKealuMREAVLRDqYmATAT/XGwVmkvspIlAKeWdOoSFEBosvLww3ePzEwTcOILqasPuI8WEBImWnVZt5tkrxzgdgvJDXaLCOVBQSkFpJTEenEEx4FoEqXus7lj3Th3scCTKn3SPjaS7fXFPqbZy97mDAFi4LdujrxNwiWDxduuAntKvi8ORKH8yb10W89ZlOR2G8jPjkzsDsHK3Z3sOBVwi+OfiXUSHhzCqZ6zToSg/8vayPby9bI/TYSg/0z8hmp6dI3lz6R6qqz13nSCgEkFpRRVF5VXERYU5HYpSSrkl0R70eriwzGOvEVCJYNdha/zAzWf0dTgSpZRyz/WnW99Xm+yy+Z4QUImgZiBZUoxe1FNK+Ya+dt2hfXklHnuNgEoEO7OtEXrDe3RyOBKllHJPzcRZ2w4c9dhrBNw4AkCvEag29+I1450OQfmpiFDr93qIB+dWD6gWwcerMwkLDiI8RAeSqbYVFxWmPzCUR4gI3TpF8GGq5+YmCKhEUFpRTXREQDaClId9mJrh0T9UFdiiI0Ioq/RczSGPJgIRmSoiW0Vkh4jc18D2cBF5396+XESSPRVLblE5BwpK+dGYHp56CRXA5qzKZM6qTKfDUH7qotHdKausJiOn2CPP77FEICLBwPPANGAYcJWIDKu3201ArjFmAPAs8BdPxbPFvtDSs7P2GFJK+Za+8VbPoZq5VNqaJ1sEE4Adxph0Y0w58B4ws94+M4E37OU5wDkiIp4IZq49/H9C3zhPPL1SSnnMmF5WJYSCkkqPPL8nT5j3AFxPmmYCJze2jzGmUkTygS7AYdedROQW4BaA3r17tyqYswcnADCoq85RrJTyLZ2jwpg6vBtd7TkK2ponE0FDv+zrF8twZx+MMbOB2QApKSmtKrhx3vBunDe8W2seqpRSjooOD+Glaz3XRdmTiSAT6OVyvydQvzxjzT6ZIhICxACen6BTqTb2+g0TnA5BqVbz5DWClcBAEekrImHALGBuvX3mAtfZy5cB3xlPT8WjlAdEhgUTGabjU5Rv8liLwD7nfzvwNRAMvGaMSRORR4BUY8xc4J/AWyKyA6slMMtT8SjlSW8t3Q3AtacmOxmGUq3i0dFVxpgvgC/qrXvIZbkUuNyTMSjVHj5bvx/QRKB8U0CNLFZKKXU8TQRKKRXgNBEopVSA00SglFIBTnytt6aIZAOtnSU8nnqjllWT9Hi1jB6vltNj1jIncrz6GGMSGtrgc4ngRIhIqjEmxek4fIUer5bR49VyesxaxlPHS08NKaVUgNNEoJRSAS7QEsFspwPwMXq8WkaPV8vpMWsZjxyvgLpGoJRS6niB1iJQSilVjyYCpZQKcH6TCERkqohsFZEdInJfA9vDReR9e/tyEUl22Xa/vX6riJzfnnE7xY3jdbeIbBKR9SLyrYj0cdlWJSJr7Vv90uJ+yY3jdb2IZLscl5+6bLtORLbbt+vqP9YfuXG8nnU5VttEJM9lWyB+vl4TkUMisrGR7SIiz9nHc72IjHPZduKfL2OMz9+wylzvBPoBYcA6YFi9fW4DXrKXZwHv28vD7P3Dgb728wQ7/Z684HidDXSwl39ec7zs+4VOvwcvPF7XA//XwGPjgHT73872cmen35PTx6ve/ndglakPyM+X/Z7PBMYBGxvZPh34EmtWx1OA5fb6Nvl8+UuLYAKwwxiTbowpB94DZtbbZybwhr08BzhHRMRe/54xpswYswvYYT+fP2v2eBlj5htjiu27y7BmmAtU7ny+GnM+8D9jTI4xJhf4HzDVQ3F6i5Yer6uAd9slMi9ljFlI07MzzgTeNJZlQKyIJNFGny9/SQQ9gAyX+5n2ugb3McZUAvlAFzcf629a+p5vwvo1UiNCRFJFZJmI/MgTAXoZd4/XpXazfY6I1EzTqp+vJt6zfcqxL/Cdy+pA+3y5o7Fj2iafL49OTNOOpIF19fvFNraPO4/1N26/ZxG5BkgBznJZ3dsYkyUi/YDvRGSDMWanB+L0Fu4cr3nAu8aYMhG5Fav1OdnNx/qblrznWcAcY0yVy7pA+3y5w6PfX/7SIsgEernc7wlkNbaPiIQAMVhNMXce62/ces8iMgV4ELjIGFNWs94Yk2X/mw4sAMZ6Mlgv0OzxMsYccTlGrwDj3X2sH2rJe55FvdNCAfj5ckdjx7RtPl9OXyRpowstIVgXSfpy7OLU8Hr7/IK6F4s/sJeHU/dicTr+f7HYneM1FuuC38B66zsD4fZyPLCdJi4E+sPNzeOV5LJ8MbDMXo4DdtnHrbO9HOf0e3L6eNn7DQZ2Yw9sDdTPl8t7T6bxi8UzqHuxeEVbfr784tSQMaZSRG4HvsbqsfCaMSZNRB4BUo0xc4F/Am+JyA6slsAs+7FpIvIBsAmoBH5h6jZT/Y6bx+spIBr40Lqmzl5jzEXAUOBlEanGalE+YYzZ5MgbaSduHq87ReQirM9QDlYvIowxOSLyKLDSfrpHjDFNXRT0eW4eL7AuEr9n7G80W8B9vgBE5F1gEhAvIpnAH4BQAGPMS1hzv0/H6sxSDNxgb2uTz5eWmFBKqQDnL9cIlFJKtZImAqWUCnCaCJRSKsBpIlBKqQCniUAppQKcJgLlVURkjIhM9+Dz3ykim0XknVY+vqYy5joRWS0ip7V1jC2IJbmmWqWITBKRz+zlixqq+NnA43eLSLy9/EMz+z7QzPYvRCTWNSZ32bGf5nL/VhH5SUueQ50YvxhHoNqXiIQYq16TJ4zBKmnxhYfiuQ2YZqwCg61RYowZY7/u+cDj1C2/0Si7yKEYY6pb+dpusfvpt6h8szGmuYT2APBY/ZUu72m6fT+2Ja9rmwQUAj/YsbzUiudQJ0BbBH7G/kW2RUTecCmA1qGB/RaIyN9E5AcR2SgiE+z1E+x1a+x/B9vrrxeRD0VkHvBfEYkWa56C1SKyQURm1nv9V+3nfUdEpojIErtees3rRNk12FfarzVTRMKAR4Ar7V/dVza0X0PxNPD+7rZff6OI/Mpe9xJWaeS5InJXA8dtkf1+3P2l3wnIdXmO39hxrheRh12ed7OIvACsBnqJSKGI/NluVSwTka72vn3sY1ozB0Rve/3rInKZy+sUNhWUfWz+r4H1XUTkv/ZxfBmXOjU1zykiSSKy0D7+G0XkDBF5Aoi0173TyHuqbV0AIQ19/uq1QFLsz2AycCtwl/38Z4jIH0XkHnu/MfYxWi8in4hIZ3v9AhH5i4isEGs+gzPc+P9SjXF6WLXe2vaGNUzdAKfb918D7mlgvwXAK/bymdhD27G+3ELs5SnAR/by9Vh1TeLs+yFAJ3s5HmvEo9ivXwmMxPqhscqOoabk93/sxzwGXGMvxwLbgCjq1fVvZr/aeOq9t/HABnu/aCANGGtv2w3EN/CYDkCEvTwQawRsQ8e3ClgLbMGqYDveXn8e1sTiYr/vz+zjmgxUA6e4PIcBLrSXnwR+Zy/PA66zl290OVavA5e5PL7Q5f+65v9tEvCZy/9VQ3MjPAc8ZC/PsOOIr/ecvwYetJeDgY6u211et/572m1/DpJp5PPneuyxWn0L7OU/4vIZdb0PrAfOspcfAf7m8vl92l6eDnzj9N+eL9+0ReCfMowxS+zlt4GJjez3LtTWQu9kN+tjsMpKbASexarFVON/5tjwdQEeE5H1wDdYpW+72tt2GWM2GOsUSBrwrbH+YjdgfVGA9cV5n4isxfqjjgB6NxBjU/u5xuNqIvCJMabIGFMIfAw094sxFHhFRDYAH2JNWNSQEmPMGGPMEKy672+KiNhxngeswfqVPAQroQDsMVYN+RrlWIkCrESZbC+fCvzbXn6Lxv/fWutMrM8DxpjPcWnNuFgJ3CAifwRGGmOONvJc9d+TK3c/f00SkRgg1hjzvb3qDaz3UONj+1/XY6haQa8R+Kf6dUMaqyPS0H6PAvONMRfbzfYFLtuLXJavBhKwfhFXiMhurC9pgDKX/apd7ldz7DMnwKXGmK2uAYjIyfViamq/IhrWUGne5twFHARGY/2iL23uAcaYpfapjgT7NR83xrxcL87kBuKssBMjWC2Mxv4Oa/aptGOqOScf1lxsTYXd5EZjForImVgthrdE5CljzJsN7NrYsW/oNY57Hxz7rJyIms9VU8dQuUFbBP6pt4icai9fBSxuZL8rAURkIpBvjMnHahHss7df38RrxACH7CRwNtCniX0b8jVwh/3FhojUlBo+CnR0Y7+mLAR+JCIdRCQKqxroomYeEwPst1sx12KdFmmSiAyx9ztix3mjiETb23qISKIbsbr6AbsYIlairfl/282xstYzsYuRtcJC+3kRkWlY1SrrEGuimEPGmFewCjXWzI1bISLuvm5jn7/dHHsfl7rsX///HAD785jrcv7/WuD7+vupE6eJwD9tBq6zT9vEAS82sl+uWN0GX8KahQysc9aPi8gSmv4yfAdIEZFUrC+XLS2M8VGsL7T19mmoR+3184Fh9oXDK5vYr1HGmNVY59VXAMuBV40xa5p52AtYx2wZMIjGf/HWXDRdC7yPdU6/yhjzX6zTOkvt00tzaODLrRl3Yp2WWY/1pfdLe/0rwFkisgJoqiXUnIeBM0VkNdZprL0N7DMJWCsia7C+rP9ur5+N9X/gTrfbxj5/DwN/F5FFWL/ia8wDLq65WFzvua4DnrKfawzWdQLVxrT6qJ+xT0V8ZowZ0cx+C7AuyKW2Q1hKKS+mLQKllApw2iJQSqkApy0CpZQKcJoIlFIqwGkiUEqpAKeJQCmlApwmAqWUCnD/D+KPszJ5BFKiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's simulate how the entropy changes for a Bernoulli random variable.\n",
    "#When p = 0 or 1, the variable is a constant and the entropy is zero.\n",
    "#When p = 0.5, the variable follows a uniform distribution and the entropy is maximum.\n",
    "\n",
    "N = 10000\n",
    "p = [x/N for x in range(N)]\n",
    "H = list()\n",
    "for i in range(len(p)):\n",
    "    pi = p[i]\n",
    "    H.append(entropy([pi, 1-pi]))\n",
    "    \n",
    "plt.plot(p,H)\n",
    "plt.xlabel(\"p parameter of a Bernoulli distribution\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.xticks([0,0.25,0.5,0.75,1])\n",
    "plt.axvline(0.5, linestyle = '--')\n",
    "plt.axhline(1, linestyle = '--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum value of the entropy was 1.00\n",
      "The predicted maximum value for the entropy was the logarithm of 2 in base 2, which equals 1.00\n"
     ]
    }
   ],
   "source": [
    "#In a Bernoulli variable, X can assume only two values, so n = 2.\n",
    "#As we antecipated, the maximum value of the entropy is equal to log_2(2):\n",
    "\n",
    "print(\"The maximum value of the entropy was {:.2f}\".format(max(H)))\n",
    "print(\"The predicted maximum value for the entropy was the logarithm of 2 in base 2, which equals {:.2f}\".format(log2(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I'm wrong (or: not all p's are created equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the girl? Of course you do! Some people are just unforgetable...\n",
    "\n",
    "She had a 1/10,000 chance of calling me sexy and a 9,999/10,000 chance of walking straight by me without even bothering to say hello. So I'm pretty sure of what will happen, and my uncertainty is small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy([1.0/10000,9999.0/10000], digits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But maybe I was wrong in believing that it was so unlikely for her to call me sexy. Maybe she loved me all along! How does that affect my entropy?\n",
    "\n",
    "Well, my surprise doesn't change! But the probabilities with which we calculate the expected value do, because they depend on the probabilities of the event actually happening, not on what I believe these probabilities are.\n",
    "\n",
    "So if I think that things happen with a probability function P, but they actually happen with a probability function Q, than the expected value of my surprise is actually\n",
    "\n",
    "$$\n",
    "H(X) = \\mathbb{E}_Q \\left[ \\log_2 \\frac{1}{p(x_i)}\\right]\n",
    "$$\n",
    "\n",
    "If we're talking about a discrete random variable, this expression becomes\n",
    "\n",
    "$$\n",
    "H(X) = \\sum_{i=1}^{N} q(x_i) \\log_2 \\frac{1}{p(x_i)}\n",
    "$$\n",
    "\n",
    "where $q(x_i)$ is the probability of the event $x_i$ \"aqtually\" happening, and $p(x_i)$ is what I \"p-lieve\" that probability to be.\n",
    "\n",
    "This entropy is called **cross entropy**. \n",
    "\n",
    "A key idea here is that the p's in the traditional entropy formula $H(X) = \\sum p \\log_2 p$ do not mean the same thing. One is the probability of things actually happening and the other is the probability with which I believe each thing will happen. That's why the latter is related to \"surprise\" but the former is related to actual expected value. \n",
    "\n",
    "This same idea appears when we define conditional entropy. Conditional entropy is the uncertainty which remains of $X$ after observing $Y$. My surprise of any given event $X$ after observing $Y$ depends on the probability $p(X|Y)$. But the probability of $X$ and $Y$ actually assuming any specific values depend on the joint probability, $p(X,Y)$. So the conditional entropy of $X$ given $Y$ is\n",
    "\n",
    "$$\n",
    "H(X|Y) = \\sum_{i=1}^{N} p(x_i,y_i) \\log_2 \\frac{1}{p(x_i|y_i)}\n",
    "$$\n",
    "\n",
    "Note that this is straightforward from the cross entropy formula, but not from the original entropy formula. This is because the original entropy formula is trecherous: it lures us into believing both p's mean the same thing. They don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposal of a new notation for cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is often represented as $H(p)$ and cross-entropy as $H(p,q)$. I find, however, that these representations are little informative, because they aren't clear as to what does $p$ and $q$ actually mean.\n",
    "\n",
    "I'll propose a new notation for the cross-entropy: $H_Q^P(X)$, where the $Q$ *below* indicates the grounded, down-to-earth, true probability distribution which is used for the expected value calculation, and the $P$ *above* indicates the probability distribution which I have on my head, on my thoughts, on my imagination, and which is therefore used for the calculation of the surprise.\n",
    "\n",
    "Under this notation, $H_P^P(X)$ is simply the good-old entropy of $X$. But it gains a clearer meaning: in this case, the probability I ascribe to $X$ (the $P$ above) is really the true probability (the $P$ below). This need not be the case, however, and the notation $H_Q^P(X)$ makes this quite clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning is all about increasing information and reducing uncertainty. Stated otherwise, to learn is to reduce surprises. It may surprise you to know that Nicole had a baby. But you would have gotten less surprised if you had known that she was pregnant. \n",
    "\n",
    "So let's say we want to learn about a random variable $X$. Your expected surprise from $X$ is the entropy of $X$, which we'll call $H(X)$. \n",
    "\n",
    "But you observed another variable $Y$ which tells you something about $X$. For example, you saw her entering a bookstore and buying a book called \"What to Expect When You're Expecting\". Now your surprise from $X$ is the entropy of $X$ *given* $Y$, which we'll call $H(X|Y)$.\n",
    "\n",
    "Learning is reducing surprises, so:\n",
    "\n",
    "$$\n",
    "H(X|Y) < H(X)\n",
    "$$\n",
    "\n",
    "The difference between these two terms is the *amount of information we gain about X by observing Y*. This has a name: **mutual information**. If we represent the mutual information of $X$ and $Y$ by $I(X,Y)$ then, by definition,\n",
    "\n",
    "$$\n",
    "I(X,Y) \\equiv H(X) - H(X|Y)\n",
    "$$\n",
    "\n",
    "I like to rewrite this as $H(X|Y) = H(X) - I(X,Y)$, which enables me to think like this: the uncertainty that remains about $X$ after having seen $Y$ is the uncertainty I had about $X$ before seeing $Y$ *minus* what I've learned about $X$ from seeing $Y$.\n",
    "\n",
    "If $X$ and $Y$ happen to be independent, one variable tells us nothing about the other and, therefore, $I(X,Y)$ should be zero. Indeed, if we learn norhing from $X$ by observing $Y$, $H(X|Y)=H(X)$, which implies $I(X,Y)=0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler divergence (Relative Entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kullback-Leibler divergence between two distributions is defined by\n",
    "\n",
    "$$\n",
    "D(P||Q) = \\sum_{i=1}^{N} p(x_i) \\log_2 \\left( \\frac{p(x_i)}{q(x_i)} \\right) \n",
    "$$\n",
    "\n",
    "It is reportedly a measure of the distance between the distributions. But it's a wacky distance, because the distance from P to Q is not the same as the distance from Q to P! So there must be a better way of understanding what this is.\n",
    "\n",
    "Let's do some maths to make things easier to grasp. A warning, first! This is my own invention, so I may be wrong. Proceed at your own peril. \n",
    "\n",
    "Rewrite $\\log_2 \\left( \\frac{p(x_i)}{q(x_i)} \\right)$ as $\\log_2 p(x_i) - \\log_2 q(x_i)$ and, subsequently, as $\\log_2 \\left( \\frac{1}{q(x_i)} \\right) - \\log_2 \\left( \\frac{1}{p(x_i)} \\right)$. Now, plug it back at the original formula for the Kulbak-Leibler divergence. With some rearranging, we get:\n",
    "\n",
    "$$\n",
    "D(P||Q) = \\left[ \\sum_{i=1}^{N} p(x_i) \\log_2 \\frac{1}{q(x_i)} \\right] - \\left[ \\sum_{i=1}^{N} p(x_i) \\log_2 \\frac{1}{p(x_i)} \\right] \n",
    "$$\n",
    "\n",
    "We now recognize two terms which are similar to the entropy and cross-entropy formulas we've seen so far. Using our notation for cross-entropy, we may write:\n",
    "\n",
    "$$\n",
    "D(P||Q) = H_P^Q(X) - H_P^P(X)\n",
    "$$\n",
    "\n",
    "Now think of entropy as a measure of my surprise from observing $X$. The second entropy measures how surprised I am when I am *right* about the true probability distribution of $X$. The first entropy measures how surprised I am when I am *wrong* about the true probability distribution of $X$ and believe it to be $Q$, rather than $P$.\n",
    "\n",
    "If we think not in terms of surprise, but in terms of information, $H_P^P(X)$ is the (expected) amount of information to be gained by observing $X$ when I am *right* about its probability distribution. Conversely, $H_P^Q(X)$ is the (expected) amount of information to be gained by observing $X$ when I am *wrong* about its probability distribution, and believe it to be $Q$ rather than $P$. **The difference between these two represents a loss in the amount of information to be gained by observing X. This loss is the Kullback-Leibler divergence.**\n",
    "\n",
    "In other words, **the Kullback-Leibler divergence measures how much (expected) information I loose by being wrong about the actual distribution of $X$, and believing it to be $Q$ rather than $P$**.\n",
    "\n",
    "It follows that $D(P||Q)$ is always positive, as one can never gain more information by being wrong. In mathematical terms,\n",
    "\n",
    "$$\n",
    "D(P||Q) \\geq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual Information (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supose $X$ and $Y$ are random variables and I use $Y$ to learn about $X$. How much information is there in $Y$ about $X$? The answer lies somewher whithin the joint probability distribution of these two variables, $p(X,Y)$. \n",
    "\n",
    "If $p(x,y)$ is equal to $p(x)\\cdot p(y)$, and $Y$ has no information about $X$.\n",
    "\n",
    "So I expect that if $p(x,y)$ is close to $p(x)\\cdot p(y)$, then $Y$ should have little information about $X$, whereas if $p(x,y)$ is very different from $p(x)\\cdot p(y)$, then $Y$ should have lots of information about $X$. \n",
    "\n",
    "Therefore, the distance between $p(x,y)$ and $p(x) \\cdot p(y)$ is a measure of how much information $Y$ carries about $X$.\n",
    "\n",
    "To put it technically, \n",
    "\n",
    "$$\n",
    "I(X,Y) = D(p(x,y) || p(x)\\cdot p(y))\n",
    "$$\n",
    "\n",
    "If we recall our explanation of the Kullback-Leibler divergence, we can easily understand why this is the case: **If we mistakenly considered the joint probability of $X$ and $Y$ to be $p(x) \\cdot p(y)$ rather than $p(x,y)$, we would loose all the information $Y$ carries about $X$.** \n",
    "\n",
    "This is because I would be treating them as independent, when they're not. If, however, they trully are independent, then $p(x,y)=p(x)\\cdot p(y)$ and $I(X,Y) = D(p(x)\\cdot p(y) || p(x)\\cdot p(y)) = 0$, as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the entropy for a two-dimensional random variable. It follows exactly the same logic, except that now, it's probability function is $p(x,y)$, rather than $p(x)$. This being said, its entropy is just what you'ld expect:\n",
    "\n",
    "$$\n",
    "H(X,Y) = \\sum p(x,y) \\log_2 \\frac{1}{p(x,y)}\n",
    "$$\n",
    "\n",
    "This is called the *joint entropy* of $X$ and $Y$. It is a fairly simple formula but it makes calculating conditional entropy much simpler.\n",
    "\n",
    "Think about it this way. We have a bidimensional random variable, $(X,Y)$. Our uncertainty about it is $H(X,Y)$. But then we learn the value of $Y$. There's no longer uncertainty on $Y$! Our uncertainty has reduced! By how much? By the uncertainty of $Y$, which is $H(Y)$. The uncertainty which remains is $H(X,Y) - H(Y)$. This is the uncertainty of $X$ *given* the value of $Y$, that is, $H(X|Y)$. It follows that\n",
    "\n",
    "$$\n",
    "H(X|Y) = H(X,Y) - H(Y)\n",
    "$$\n",
    "\n",
    "We can read this like this: **the uncertainty that remains of $X$ after knowing $Y$ is the total uncertainty of $X$ and $Y$ *minus* the uncertainty which we've removed by getting to know $Y$.**\n",
    "\n",
    "This is a much easier way to calculate $H(X|Y)$ than the way we've previously seen. \n",
    "\n",
    "Now recall that $I(X,Y) = H(X) - H(X|Y)$ or, equivalently, $H(X|Y) = H(X) - I(X,Y)$. It follows that $H(X) - I(X,Y) = H(X,Y) - H(Y)$, which yields\n",
    "\n",
    "$$\n",
    "I(X,Y) = H(X) + H(Y) - H(X,Y)\n",
    "$$\n",
    "\n",
    "that is to say, the total uncertainty I get from considering $X$ and $Y$ independently is actually greater than if I consider $X$ and $Y$ conjointly, because when I consider them conjointly and account for the fact that the behavior of one variable restraints the behavior of the other variable, so this reduces a bit my uncertainty. This reduction is simply the information I get from one variable by observing the other, that is, the mutual information. \n",
    "\n",
    "\n",
    "This is not the first time we see mutual information between two variables being associated with the difference between considering separately of conjointly. We have seen this before, when we studied the Kullback-Leibler divergence and saw that\n",
    "\n",
    "$$\n",
    "I(X,Y) = D(p(x,y) || p(x)\\cdot p(y))\n",
    "$$\n",
    "\n",
    "Beautiful, isn't it?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
